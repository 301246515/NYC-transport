{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = json.load(open('config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def assign_taxi_zones(df, lon_var, lat_var, locid_var):\n",
    "    \"\"\"Joins DataFrame with Taxi Zones shapefile.\n",
    "\n",
    "    This function takes longitude values provided by `lon_var`, and latitude\n",
    "    values provided by `lat_var` in DataFrame `df`, and performs a spatial join\n",
    "    with the NYC taxi_zones shapefile. \n",
    "\n",
    "    The shapefile is hard coded in, as this function makes a hard assumption of\n",
    "    latitude and longitude coordinates. It also assumes latitude=0 and \n",
    "    longitude=0 is not a datapoint that can exist in your dataset. Which is \n",
    "    reasonable for a dataset of New York, but bad for a global dataset.\n",
    "\n",
    "    Only rows where `df.lon_var`, `df.lat_var` are reasonably near New York,\n",
    "    and `df.locid_var` is set to np.nan are updated. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame or dask.DataFrame\n",
    "        DataFrame containing latitudes, longitudes, and location_id columns.\n",
    "    lon_var : string\n",
    "        Name of column in `df` containing longitude values. Invalid values \n",
    "        should be np.nan.\n",
    "    lat_var : string\n",
    "        Name of column in `df` containing latitude values. Invalid values \n",
    "        should be np.nan\n",
    "    locid_var : string\n",
    "        Name of column in `df` containing taxi_zone location ids. Rows with\n",
    "        valid, nonzero values are not overwritten. \n",
    "    \"\"\"\n",
    "\n",
    "    import geopandas\n",
    "    from shapely.geometry import Point\n",
    "\n",
    "\n",
    "    localdf = df[[lon_var, lat_var, locid_var]].copy()\n",
    "    # localdf = localdf.reset_index()\n",
    "    localdf[lon_var] = localdf[lon_var].fillna(value=0.)\n",
    "    localdf[lat_var] = localdf[lat_var].fillna(value=0.)\n",
    "    localdf['replace_locid'] = (localdf[locid_var].isnull()\n",
    "                                & (localdf[lon_var] != 0.)\n",
    "                                & (localdf[lat_var] != 0.))\n",
    "\n",
    "    if (np.any(localdf['replace_locid'])):\n",
    "        shape_df = geopandas.read_file('../shapefiles/taxi_zones_latlon.shp')\n",
    "        shape_df.drop(['OBJECTID', \"Shape_Area\", \"Shape_Leng\", \"borough\", \"zone\"],\n",
    "                      axis=1, inplace=True)\n",
    "\n",
    "        try:\n",
    "            local_gdf = geopandas.GeoDataFrame(\n",
    "                localdf, crs={'init': 'epsg:4326'},\n",
    "                geometry=[Point(xy) for xy in\n",
    "                          zip(localdf[lon_var], localdf[lat_var])])\n",
    "\n",
    "            local_gdf = geopandas.sjoin(\n",
    "                local_gdf, shape_df, how='left', op='intersects')\n",
    "\n",
    "            # one point can intersect more than one zone -- for example if on\n",
    "            # the boundary between two zones. Deduplicate by taking first valid.\n",
    "            local_gdf = local_gdf[~local_gdf.index.duplicated(keep='first')]\n",
    "\n",
    "            local_gdf.LocationID.values[~local_gdf.replace_locid] = (\n",
    "                (local_gdf[locid_var])[~local_gdf.replace_locid]).values\n",
    "\n",
    "            return local_gdf.LocationID.rename(locid_var)\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            print(ve.stacktrace())\n",
    "            return df[locid_var]\n",
    "    else:\n",
    "        return df[locid_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uber():\n",
    "    uber_schema_2014=\"pickup_datetime,pickup_latitude,pickup_longitude,junk1\"\n",
    "    uber_glob_2014 = glob(os.path.join(config['uber_raw_data_path'],'uber*-???14.csv'))\n",
    "\n",
    "    uber_schema_2015=\"junk1,pickup_datetime,junk2,pickup_location_id\"\n",
    "    uber_glob_2015 = glob(os.path.join(config['uber_raw_data_path'],'uber*15.csv'))\n",
    "\n",
    "    dtype_list = { \n",
    "        # 'dropoff_datetime': np.int64,\n",
    "        'dropoff_latitude': np.float64,\n",
    "        'dropoff_location_id': np.int64,\n",
    "        'dropoff_longitude': np.float64,\n",
    "        'ehail_fee': np.float64,\n",
    "        'extra': np.float64,\n",
    "        'fare_amount': np.float64,\n",
    "        'improvement_surcharge': np.float64,\n",
    "        'junk1': object,\n",
    "        'junk2': object,\n",
    "        'mta_tax': np.float64,\n",
    "        'passenger_count': np.int64,\n",
    "        'payment_type': object,\n",
    "    #     'pickup_datetime': object, # set by parse_dates in pandas read_csv\n",
    "        'pickup_latitude': np.float64,\n",
    "        'pickup_location_id': np.int64,\n",
    "        'pickup_longitude': np.float64,\n",
    "        'rate_code_id': np.int64,\n",
    "        'store_and_fwd_flag': object,\n",
    "        'tip_amount': np.float64,\n",
    "        'tolls_amount': np.float64,\n",
    "        'total_amount': np.float64,\n",
    "        'trip_distance': np.float64,\n",
    "        'trip_type': object,\n",
    "        'vendor_id': object\n",
    "    }\n",
    "\n",
    "    uber1 = dd.read_csv(uber_glob_2014, header=0,\n",
    "                         na_values=[\"NA\"], \n",
    "                         parse_dates=[0,],\n",
    "                         infer_datetime_format = True,\n",
    "                         dtype=dtype_list,\n",
    "                         names=uber_schema_2014.split(','))\n",
    "    uber1 = uber1.drop(['junk1',], axis=1)\n",
    "    uber1 = uber1.assign(pickup_location_id=-999)\n",
    "\n",
    "    uber2 = dd.read_csv(uber_glob_2015, header=0,\n",
    "                         na_values=[\"NA\"], \n",
    "                         parse_dates=[1,],\n",
    "                         infer_datetime_format = True,\n",
    "                         dtype=dtype_list,\n",
    "                         names=uber_schema_2015.split(','))\n",
    "    uber2 = uber2.drop(['junk1', 'junk2'], axis=1)\n",
    "    uber2 = uber2.assign(pickup_latitude=np.nan, pickup_longitude=np.nan)\n",
    "\n",
    "    uber1 = uber1[sorted(uber1.columns)]\n",
    "    uber2 = uber2[sorted(uber2.columns)]\n",
    "\n",
    "    uberdf = uber1.append(uber2)\n",
    "\n",
    "    default_values = {np.float64: np.nan, np.int64: -999, object: \"\"}\n",
    "\n",
    "\n",
    "    for field in dtype_list:\n",
    "        if (field in uberdf.columns):\n",
    "            uberdf[field] = uberdf[field].astype(dtype_list[field])\n",
    "        elif field == 'pickup_datetime':\n",
    "            pass\n",
    "        else:\n",
    "            uberdf = uberdf.assign(**{field: default_values[dtype_list[field]]})\n",
    "\n",
    "\n",
    "    uberdf = uberdf.drop(['junk1', 'junk2'], axis=1)\n",
    "\n",
    "    uberdf['dropoff_datetime'] = np.datetime64(\"1970-01-01 00:00:00\")\n",
    "    #uberdf = uberdf.repartition(npartitions=20)\n",
    "\n",
    "    uberdf['trip_type'] = 'uber'\n",
    "\n",
    "    uberdf = uberdf[sorted(uberdf.columns)]\n",
    "\n",
    "    return uberdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = get_uber()\n",
    "\n",
    "u['dropoff_location_id'] = u.map_partitions(\n",
    "    assign_taxi_zones, \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "    \"dropoff_location_id\", meta=('dropoff_location_id', np.float64))\n",
    "u['pickup_location_id'] = u.map_partitions(\n",
    "    assign_taxi_zones, \"pickup_longitude\", \"pickup_latitude\",\n",
    "    \"pickup_location_id\", meta=('pickup_location_id', np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1026663</th>\n",
       "      <td>2015-05-08 15:43:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026664</th>\n",
       "      <td>2015-05-08 15:43:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026665</th>\n",
       "      <td>2015-05-08 15:43:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026666</th>\n",
       "      <td>2015-05-08 15:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026667</th>\n",
       "      <td>2015-05-08 15:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickup_datetime  pickup_latitude  pickup_longitude  \\\n",
       "1026663 2015-05-08 15:43:00              NaN               NaN   \n",
       "1026664 2015-05-08 15:43:00              NaN               NaN   \n",
       "1026665 2015-05-08 15:43:00              NaN               NaN   \n",
       "1026666 2015-05-08 15:44:00              NaN               NaN   \n",
       "1026667 2015-05-08 15:44:00              NaN               NaN   \n",
       "\n",
       "         pickup_location_id  \n",
       "1026663                 186  \n",
       "1026664                 263  \n",
       "1026665                  90  \n",
       "1026666                  45  \n",
       "1026667                 144  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u['pickup_datetime,pickup_latitude,pickup_longitude,pickup_location_id'.split(',')].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u.to_csv('/bigdata/csv/uber-*.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
